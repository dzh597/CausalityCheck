# CausalityCheck



**CausalityCheck: A  Framework for Evaluating Causal Reasoning in Large Language Models**


## Intro

Exceptional mathematical reasoning ability is one of the key features that demonstrate the power of large language models (LLMs). How to comprehensively define and evaluate the mathematical abilities of LLMs, and even reflect the user experience in real-world scenarios, has emerged as a critical issue. Current benchmarks predominantly concentrate on problem-solving capabilities, which presents a substantial risk of model overfitting and fails to accurately represent genuine mathematical reasoning abilities.

In this paper, we argue that if a model really understands a problem, it should be robust and readily applied across a variety of tasks. Motivated by this, we introduce **MATHCHECK**, a well-designed checklist for testing test generation and reasoning capabilities, and an automatic tool to generate checklists efficiently. **MATHCHECK** utilizes multi-task mathematical reasoning and robustness test types for evaluating the capabilities of large models.

### Key Features
- Evaluates modelâ€™s mathematical reasoning ability
- Generates multiple tasks automatically
- Provides a comprehensive evaluation of math reasoning performance

## Dataset

Here, you can provide a link or description of the dataset used for the project.

[Dataset Link](https://example.com/dataset)

## Results

| Model        | Test 1 Accuracy | Test 2 Accuracy | Test 3 Accuracy |
|--------------|-----------------|-----------------|-----------------|
| Model A      | 85%             | 88%             | 90%             |
| Model B      | 78%             | 80%             | 85%             |

## Installation

To use this project, you can clone the repository:

```bash
git clone https://github.com/dzh597/CausalityCheck.git
cd CausalityCheck
